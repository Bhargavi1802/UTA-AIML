{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### Personal Loan Campaign Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal of the project is to predict the likelihood of a liabiliy customer to convert to buy personal loan with the bank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt# matplotlib.pyplot plots data\n",
    "%matplotlib inline \n",
    "from sklearn.model_selection import train_test_split\n",
    "import missingno as msno\n",
    "import zipcodes as zc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('therabankdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe().transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.nunique()  ## Unique values is in each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Basic checks on the data before getting it ready for analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_checks(df):\n",
    "    \n",
    "    print('='*50)\n",
    "    print('Shape of the dataframe is: \\n',df.shape)\n",
    "    print('='*50)\n",
    "    print('Basic stats for the data: \\n',df.describe())\n",
    "    print('='*50)\n",
    "    print('Data type and info :')\n",
    "    print(df.info())\n",
    "    print('='*50)\n",
    "    print('Missing value information : \\n',df.isnull().any())\n",
    "    print('='*50)\n",
    "    print('Sum of missing values if any : \\n',df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_checks(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Missing values matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolutely no missing values in the data shown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Zero mortgage and zero credit spend from the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mortgagezero=data['Mortgage']==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mortgagezero.value_counts() ## 3462 records have no mortgage on top of the loan ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximately 70% of the customers have no mortgage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mortgagezero1=data[(data['Mortgage']==0)&(data['Personal Loan']==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mortgagezero1['Mortgage'].value_counts()/Mortgagezero.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20.29 % of those who dont have a mortgage have opted for a personal loan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit spend is zero "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditspendzero=data['CCAvg']==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditspendzero.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 % of the customers dont spend using their credit cards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA - Part 1 - Summary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.5000 records and 14 columns in the dataset.\n",
    "\n",
    "2.Summary stats show that :\n",
    "\n",
    "a.People between the age range of 23-67 have been targeted for the campaign, the average age range is 45.\n",
    "b.Customers have average work experience of 20 years ,with Income ranging from 8k to 224k .\n",
    "c.The average family size of the customer is 2.3, with an Credit card spend of 1.93k .\n",
    "d.The customer base are not highly educated ,they mostly fit in the level 1 and 2 of education.\n",
    "e.Mortgage values range from 0-635k USD.\n",
    "\n",
    "3.Null value checks show that there are no null values in the data.\n",
    "\n",
    "4.Approximately 70 % of the customers have no mortgage ,20 % of those that have no mortgage have opted for a personal loan in the campaign last year .This could be indicative of a good opportunity for the bank in targeting these customers as they could prove to be low risk.\n",
    "\n",
    "5.2% of the customer base doesn’t spend using the credit cards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping ID as it adds no value to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['ID'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Data Cleaning \n",
    "\n",
    "Some level of data cleaning is required on the data set which require the following tasks to be done \n",
    "\n",
    "1. ID column needs to be dropped \n",
    "2. ZIP code needs to be converted so that some meaningful insights  can be generated from it .\n",
    "3. Education needs to be converted from numeric to categorical variable\n",
    "4. Experience columns shows negative values which need to be converted to the corresponding positive values assuming they were data entry issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['Experience'].replace([-3,-2,-1],[3,2,1],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_checks(data1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Processing the zip codes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zc.matching('94143')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['ZIP Code']=data1['ZIP Code'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zipcode_state(x):\n",
    "    l= zc.matching(x)\n",
    "    if len(l)>0:\n",
    "        state=l[0]['state']\n",
    "    else :\n",
    "        state='Unknown'\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['state']=data1['ZIP Code'].map(zipcode_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['state'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zipcode_city(x):\n",
    "    l= zc.matching(x)\n",
    "    if len(l)>0:\n",
    "        City=l[0]['city']\n",
    "    else :\n",
    "        City='Unknown'\n",
    "    return City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['city']=data1['ZIP Code'].map(zipcode_city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['city'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Education - making it a categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['Education']=data1['Education'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Value counts for categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valuecounts(variable):\n",
    "        print(data1[variable].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valuecounts('ZIP Code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valuecounts('city')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valuecounts('state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valuecounts('Education')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Univariate & Bi variate analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plotting the correlations between the variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# However we want to see correlation in graphical representation so below is function for that\n",
    "def plot_corr(df, size=11):\n",
    "    corr = df.corr()\n",
    "    fig, ax = plt.subplots(figsize=(size, size))\n",
    "    ax.matshow(corr)\n",
    "    plt.xticks(range(len(corr.columns)), corr.columns)\n",
    "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
    "    for (i, j), z in np.ndenumerate(corr):\n",
    "        ax.text(j, i, '{:0.1f}'.format(z), ha='center', va='center')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corr(data1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Correlation pairplot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_data=data1.iloc[:,0:12]\n",
    "sns.pairplot(corr_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Univariate & Bivariate plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plots(variable):\n",
    "    fig=plt.figure(figsize=(10,5))\n",
    "    plt.subplot(131)\n",
    "    sns.distplot(data1[variable])\n",
    "    plt.subplot(132)\n",
    "    sns.boxplot(x=data1[variable])\n",
    "    plt.subplot(133)\n",
    "    sns.boxplot(x=data1['Personal Loan'],y=data1[variable])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age - Bivariate and Univariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots('Age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experience - Bivariate and Univariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots('Experience')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Income - Bivariate and Univariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots('Income')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CCAvg - Bivariate and Univariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots('CCAvg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mortgage - Bivariate and Univariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots('Mortgage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Countplots for the categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catplot(variable):\n",
    "    fig=plt.figure(figsize=(10,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    sns.countplot(x=variable,data=data1)\n",
    "    plt.subplot(1,2,2)\n",
    "    sns.countplot(x=variable, hue='Personal Loan', data=data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catplot('Family')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catplot('Education')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Personal Loan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='Personal Loan',data=data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='Securities Account',hue='Personal Loan',data=data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='CD Account',hue='Personal Loan',data=data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='Online',hue='Personal Loan',data=data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='CreditCard',hue='Personal Loan',data=data1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zip Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which STATE has the highest customer base ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='state',hue='Personal Loan',data=data1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "California state is  the customer base for the bank .Lets look at the breakdown by City."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 cities in California with customer base in the bank ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1[['city','Personal Loan']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=data1['city'].value_counts(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['city'].nunique() ## Number of unique cities in the dataset is 245."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA part -2 \n",
    "\n",
    "##### Correlation :\n",
    "\n",
    "1. Age and Experience seem to be highly correlated \n",
    "2. Personal loan and Income seem to be somewhat correlated .\n",
    "3. We don’t see any significant correlations between the variables\n",
    "\n",
    "###### Univariate and bivariate plots :\n",
    "\n",
    "###### Numerical variables :\n",
    "\n",
    "1.Age shows some what normal distribution.For those that haven’t opted for personal loan the age range is between 20-60 and for those that have opted for a personal loan range from mid twenties to a bit over 60 .No outliers seen.\n",
    "\n",
    "2.Income shows right skewed data.There seems to be outliers in the data for those that did not opt for personal loan, the data shows outliers greater than 150k .For those that opted the personal loan, there are no outliers and the data ranges from 60 k -200k\n",
    "\n",
    "3.CCAvg is right skewed data ,there seems to be outliers in the data, which show spend greater than 5k .Those that have a personal loan have a more diverse range of CC Average and the median spend seems to clearly higher for those that opted for personal loan.\n",
    "\n",
    "4.Mortgage is also right skewed.There are plenty of outliers in this data too.The IQR range is higher for those that opted personal loan.\n",
    "\n",
    "***** Outliers seemed to have to effect on the overall model (checked this and struck the option off) and hence they were not treated while trying to clean the data .******\n",
    "\n",
    "\n",
    "\n",
    "###### Categorical variables :\n",
    "\n",
    "1.Zip code was converted to categorical from numerical to extract city and state from the dataset .CA /California has the highest number of customers ,the top 5 cities with customers are Loas Angeles,San Diego,San Francisco,Berkeley,Sacramento . The customers are from 245 different cities .\n",
    "\n",
    "2.The most popular family sizes are singles and couples.Those who did opt personal loan the family size seems to be around 3.\n",
    "\n",
    "3.The customer base shows that most have basic educational qualification followed by people who have an advanced qualification.\n",
    "\n",
    "4.Personal loans were granted to customers that have higher education qualification.\n",
    "\n",
    "5.11% of Securities account holders have a personal loan\n",
    "\n",
    "6.46 % of CD Account holders have a personal loan \n",
    "\n",
    "7.Online - 9 % of those that use the internet banking have a personal loan\n",
    "\n",
    "8 .Credit card - Approx 10 % of those that have a credit card with the bank have a personal loan \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependant variable distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 - Customers that did not opt for a personal loan in the previous campaign\n",
    "1 - Customers that did opt for a personal loan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['Personal Loan'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "90.4 % did not opt for a personal loan ,9.6 % opted for a personal loan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='Personal Loan',data=data1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changes to other attributes with respect to the dependant variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.groupby(['Personal Loan']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.groupby(['Personal Loan']).median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inferences:\n",
    "\n",
    "1. 90.4% of the customers don’t opt for personal loan,9.6 % of the customers opt for personal loan.\n",
    "2. The average age for those that opt for personal loan and those that don’t is 45.The average Experience for the customers who opt for personal loan in 19.8\n",
    "3. The average  Family size is 2.6 for those that opt for personal loan and the credit card average is 3.9K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting the data ready for model building :\n",
    "\n",
    "1. Applying ***One Hot Encoding*** to the Education variable .\n",
    "2. Dropping the variables ‘City’ and ‘State’ from the dataset as we have done the initial round of analysis on the Zip code .\n",
    "3. Drop ZIP Code from the dataset .\n",
    "\n",
    "NB : ***The model was run with both ZIP code and without ZIP Code as a variable ,adding ZIPCode only adds numerous more variables after applying one hot encoding and it added no value to the model performance .It was then decided to drop ‘ZIP Code’ only to see the model performance unchanged.***\n",
    "\n",
    "4. Split the data into train and test data sets  in 70 ,30 ratio.\n",
    "\n",
    "5. Once the data is ready to be used in the model apply the classifier ,Logistic Regression from sklearn.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=data1.drop(['city', 'state'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping the ZIP Code from the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2=data1.drop(['ZIP Code'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2=pd.get_dummies(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split the data into train and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data2.drop('Personal Loan',axis=1)\n",
    "Y=data2['Personal Loan']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{0:0.2f}% data is in training set\".format((len(x_train)/len(data2.index)) * 100))\n",
    "print(\"{0:0.2f}% data is in test set\".format((len(x_test)/len(data2.index)) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original Personal Loan True Values    : {0} ({1:0.2f}%)\".format(len(data2.loc[data2['Personal Loan'] == 1]), (len(data2.loc[data2['Personal Loan'] == 1])/len(data2.index)) * 100))\n",
    "print(\"Original Personal Loan False Values   : {0} ({1:0.2f}%)\".format(len(data2.loc[data2['Personal Loan'] == 0]), (len(data2.loc[data2['Personal Loan'] == 0])/len(data2.index)) * 100))\n",
    "print(\"\")\n",
    "print(\"Training Personal Loan True Values    : {0} ({1:0.2f}%)\".format(len(y_train[y_train[:] == 1]), (len(y_train[y_train[:] == 1])/len(y_train)) * 100))\n",
    "print(\"Training Personal Loan False Values   : {0} ({1:0.2f}%)\".format(len(y_train[y_train[:] == 0]), (len(y_train[y_train[:] == 0])/len(y_train)) * 100))\n",
    "print(\"\")\n",
    "print(\"Test Personal Loan True Values        : {0} ({1:0.2f}%)\".format(len(y_test[y_test[:] == 1]), (len(y_test[y_test[:] == 1])/len(y_test)) * 100))\n",
    "print(\"Test Personal Loan False Values       : {0} ({1:0.2f}%)\".format(len(y_test[y_test[:] == 0]), (len(y_test[y_test[:] == 0])/len(y_test)) * 100))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling - Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Build a basic Logistic regression with default parameters.Note the accuracy, recall, precision,F1 score and ROC AUC score.\n",
    "2. Tweak the parameters - C,Solver, to see any improvement in the model performance.\n",
    "3. Balance the data to improve the performance measures.\n",
    "4. Compare the performance measures, and pick the model that gives us the best Recall,ROC AUC scores ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "#from sklearn import confusion_matrix,recall_score,precision_score,f1_score,roc_auc_score,accuracy_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "## Fitting the model on training dataset \n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(x_train,y_train)\n",
    "\n",
    "## Predicting on test set \n",
    "y_predict=model.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inferences:\n",
    "\n",
    "The above gives us the classifier with the default parameters apart from 'Solver' which is mentioned as 'liblinear'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Model Scores on Train & Test Data set ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_score=model.score(x_test,y_test)\n",
    "print(model_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_score_train=model.score(x_train,y_train)\n",
    "print(model_score_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train and test datasets show good model score .Lets us now print the confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix=metrics.confusion_matrix(y_test,y_predict)\n",
    "print('The Confusion Matrix is displayed below :')\n",
    "print('')\n",
    "\n",
    "print(confusion_matrix)\n",
    "print('')\n",
    "\n",
    "\n",
    "## Confusion Matrix\n",
    "\n",
    "cm=metrics.confusion_matrix(y_test,y_predict)\n",
    "sns.heatmap(cm,annot=True,  fmt='.2f', xticklabels = [0,1] , yticklabels = [0,1])\n",
    "plt.ylabel('Observed')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test,y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has high precision low recall and an OK f1 score. We would like to see a high recall score ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy,Precision,Recall,ROC_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accuracy\n",
    "\n",
    "print('Training Accuracy is :',model.score(x_train,y_train))\n",
    "print('')\n",
    "     \n",
    "print('Testing Accuracy is:',model.score(x_test,y_test))\n",
    "print('')\n",
    "\n",
    "## Precision\n",
    "print('Precision of the model is :',metrics.precision_score(y_test,y_predict))\n",
    "print('')\n",
    "\n",
    "##Recall \n",
    "print('Recall of the model is:',metrics.recall_score(y_test,y_predict))\n",
    "print('')\n",
    "\n",
    "##F1 score \n",
    "print('The F1 score is:',metrics.f1_score(y_test,y_predict))\n",
    "print('')\n",
    "\n",
    "##ROC Auc score \n",
    "print('The ROC auc score is :',metrics.roc_auc_score(y_test,y_predict))\n",
    "print('')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC/AUC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "roc_auc_score(y_test,y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_roc_auc = roc_auc_score(y_test, model.predict(x_test))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(x_test)[:,1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improving Model performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting the regression model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score=[]\n",
    "test_score=[]\n",
    "solver=['newton-cg','lbfgs','liblinear','sag', 'saga']\n",
    "\n",
    "for i in solver:\n",
    "    model=LogisticRegression(random_state=None,penalty='l2', C = 1,solver=i)\n",
    "    model.fit(x_train,y_train)\n",
    "    y_predict=model.predict(x_test)\n",
    "    train_score.append(round(model.score(x_train,y_train),2))\n",
    "    test_score.append(round(model.score(x_test,y_test),2))\n",
    "    \n",
    "print(solver)\n",
    "print()\n",
    "print(train_score)\n",
    "print()\n",
    "print(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### picking 'liblinear' solver ,we pick this and proceed\n",
    "##with tweaking the C value  to see if it improves the recall and precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model \n",
    "\n",
    "model=LogisticRegression(random_state=None,penalty='l2', C = 1,solver='liblinear')\n",
    "model.fit(x_train,y_train)\n",
    "y_predict=model.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score=[]\n",
    "test_score=[]\n",
    "\n",
    "C=[0.01,0.1,0.25,0.5,0.75,1]\n",
    "\n",
    "for i in C :\n",
    "    model=LogisticRegression(random_state=None,penalty='l2', C = i ,solver='liblinear')\n",
    "    model.fit(x_train,y_train)\n",
    "    y_predict=model.predict(x_test)\n",
    "    train_score.append(round(model.score(x_train,y_train),3))\n",
    "    test_score.append(round(model.score(x_test,y_test),3))\n",
    "  \n",
    "print(C)\n",
    "print()\n",
    "print(train_score)\n",
    "print()\n",
    "print(test_score)\n",
    "\n",
    "#print(metrics.f1_score(y_test,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accuracy\n",
    "model=LogisticRegression(random_state=None,penalty='l2',solver='liblinear')\n",
    "model.fit(x_train,y_train)\n",
    "y_predict=model.predict(x_test)\n",
    "\n",
    "print('Training Accuracy is :',model.score(x_train,y_train))\n",
    "print('')\n",
    "     \n",
    "print('Testing Accuracy is:',model.score(x_test,y_test))\n",
    "print('')\n",
    "\n",
    "## Precision\n",
    "print('Precision of the model is :',metrics.precision_score(y_test,y_predict))\n",
    "print('')\n",
    "\n",
    "##Recall \n",
    "print('Recall of the model is:',metrics.recall_score(y_test,y_predict))\n",
    "print('')\n",
    "\n",
    "##F1 score \n",
    "print('The F1 score is:',metrics.f1_score(y_test,y_predict))\n",
    "print('')\n",
    "\n",
    "##ROC Auc score \n",
    "print('The ROC auc score is :',metrics.roc_auc_score(y_test,y_predict))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusionmatrix=metrics.confusion_matrix(y_test,y_predict)\n",
    "print('The Confusion Matrix is displayed below :')\n",
    "print('')\n",
    "\n",
    "print(confusionmatrix)\n",
    "print('')\n",
    "\n",
    "\n",
    "## Confusion Matrix\n",
    "\n",
    "cm=metrics.confusion_matrix(y_test,y_predict)\n",
    "sns.heatmap(cm,annot=True,  fmt='.2f', xticklabels = [0,1] , yticklabels = [0,1])\n",
    "plt.ylabel('Observed')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The true positives show slight improvement ,along with the Recall,F1,ROC/AUC score. We will hence stick to the parameters used and then use 'class_weight'= 'balanced' as our data seems to be highly imbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Business Insights`\n",
    "\n",
    "True Negative (observed=0,predicted=0)\n",
    "\n",
    "Predicted that the customer would not accept the personal loan and they actually do not .\n",
    "\n",
    "False Positive (observed=0,predicted=1)\n",
    "\n",
    "Predicted that the customer would accept the personal loan while the customer did not.\n",
    "\n",
    "True Negative (observed=0,predicted=0)\n",
    "\n",
    "Predicted that the customer would not accept the personal loan and the customer did not.\n",
    "\n",
    "False Negative(observed=1,predicted=0)\n",
    "\n",
    "Predicted the customer would not accept the loan when the customer did.\n",
    "\n",
    "`Metrics of main interest`\n",
    "\n",
    "In the 1st model we get an accuracy which is greater than 90% ,however we would like to focus on the metrics - Precision and F1 score here .\n",
    "\n",
    "The False negatives in this case are missed opportunity to make money for the bank .Hence the lower the number of False negatives the better.\n",
    "\n",
    "We therefore have the model tuned so it gives us an optimal F1 score,recall and precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Treating the imbalance in the data by tweaking the class_weight parameter ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After changing the parameters ,adjusting the class_weight parameter to 'balanced',so the model performance can be made better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accuracy\n",
    "model=LogisticRegression(random_state=None,penalty='l2',solver='liblinear',class_weight='balanced')\n",
    "model.fit(x_train,y_train)\n",
    "y_predict=model.predict(x_test)\n",
    "\n",
    "print('Training Accuracy is :',model.score(x_train,y_train))\n",
    "print('')\n",
    "     \n",
    "print('Testing Accuracy is:',model.score(x_test,y_test))\n",
    "print('')\n",
    "\n",
    "## Precision\n",
    "print('Precision of the model is :',metrics.precision_score(y_test,y_predict))\n",
    "print('')\n",
    "\n",
    "##Recall \n",
    "print('Recall of the model is:',metrics.recall_score(y_test,y_predict))\n",
    "print('')\n",
    "\n",
    "##F1 score \n",
    "print('The F1 score is:',metrics.f1_score(y_test,y_predict))\n",
    "print('')\n",
    "\n",
    "##ROC Auc score \n",
    "print('The ROC auc score is :',metrics.roc_auc_score(y_test,y_predict))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusionmatrix=metrics.confusion_matrix(y_test,y_predict)\n",
    "print('The Confusion Matrix is displayed below :')\n",
    "print('')\n",
    "\n",
    "print(confusionmatrix)\n",
    "print('')\n",
    "\n",
    "\n",
    "## Confusion Matrix\n",
    "\n",
    "cm=metrics.confusion_matrix(y_test,y_predict)\n",
    "sns.heatmap(cm,annot=True,  fmt='.2f', xticklabels = [0,1] , yticklabels = [0,1])\n",
    "plt.ylabel('Observed')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "` Business Insights `\n",
    "\n",
    "True Negative (observed=0,predicted=0)\n",
    "\n",
    "Predicted that the customer would not accept the personal loan and they actually do not .\n",
    "\n",
    "False Positive (observed=0,predicted=1)\n",
    "\n",
    "Predicted that the customer would accept the personal loan while the customer did not.\n",
    "\n",
    "True Negative (observed=0,predicted=0)\n",
    "\n",
    "Predicted that the customer would not accept the personal loan and the customer did not.\n",
    "\n",
    "False Negative(observed=1,predicted=0)\n",
    "\n",
    "Predicted the customer would not accept the loan when the customer did.\n",
    "\n",
    "`Metrics of main interest`\n",
    "\n",
    "***Our fundamental assumption here is that we are more interested in figuring customers out that convert as personal loan customers,hence high recall and AUC score should be our metrics***\n",
    "\n",
    "Since accuracy is not our main metric ,so we look at the other numbers.\n",
    "\n",
    "The ***False negatives*** in this case are much lower than the previous model ,indicating that we reduce our 'missed opportunity' to get a 'much better' conversion for the personal loan,thus increasing the income for the bank.\n",
    "\n",
    "The ***True positives*** number has also gone up in the current model,which means that the  predictions made that the customer would buy a personal loan matches the actual.\n",
    "\n",
    "The ***False Positives number*** has also gone up here ,however missing this could be relatively harmless .Precision goes down as the False positives have gone up.\n",
    "\n",
    "It is worth noting here that the Accuracy has gone down by a small % ,while Recall and ROC AUC values have shown a significant increase.\n",
    "\n",
    "Although F1 score and precision have gone down ,our focus is on the Recall and AUC values as these highlight the number of False Negatives and True positives and these show a significant increase . We will therefore finalize this as our final model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
